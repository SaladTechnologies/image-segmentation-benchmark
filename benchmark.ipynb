{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "from typing import Generator, Dict, Any\n",
    "import itertools\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import os\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import pandas as pd\n",
    "import ast\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "boto3.setup_default_session(profile_name='r2')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "image_bucket = \"salad-benchmark-public-assets\"\n",
    "bucket_domain = \"https://salad-benchmark-assets.download\"\n",
    "\n",
    "salad_green = \"#53a626\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_file_urls(bucket: str, prefix: str) -> Generator[str, None, None]:\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "    for page in page_iterator:\n",
    "        for content in page['Contents']:\n",
    "            yield f\"{bucket_domain}/{content['Key']}\"\n",
    "            \n",
    "def create_session(max_retries=3, backoff_factor=0.3):\n",
    "    \"\"\"\n",
    "    Create a requests session with retry strategy.\n",
    "    :param max_retries: Maximum number of retries for each request.\n",
    "    :param backoff_factor: A backoff factor to apply between attempts.\n",
    "    :param status_forcelist: A set of HTTP status codes that we should force a retry on.\n",
    "    :return: A requests session object.\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=max_retries,\n",
    "                    read=max_retries,\n",
    "                    connect=max_retries,\n",
    "                    backoff_factor=backoff_factor,\n",
    "                    status_forcelist=[i for i in range(500, 600)],\n",
    "                    respect_retry_after_header=True\n",
    "                    )\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "# Usage\n",
    "session = create_session()\n",
    "            \n",
    "num_requests = 0\n",
    "num_responses = 0\n",
    "list_of_all_requests = []\n",
    "def http_worker() -> requests.Response:\n",
    "    \"\"\" Function to make an HTTP request \"\"\"\n",
    "    global num_requests, num_responses, list_of_all_requests\n",
    "    num_requests += 1\n",
    "    request_params = list_of_all_requests.pop()\n",
    "    print(f\"\\r{num_requests} submitted | {num_responses} responses\", end=\"\", flush=True)\n",
    "    try:\n",
    "        response = session.request(**request_params)\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\", flush=True)\n",
    "        response = None\n",
    "    num_responses += 1\n",
    "    return response\n",
    "      \n",
    "def chunk_list(lst, chunk_size):\n",
    "    \"\"\"Yield successive chunks of chunk_size from lst.\"\"\"\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i:i + chunk_size]\n",
    "\n",
    "def fetch_responses(pool_size: int = 5) -> Generator[requests.Response, None, None]:\n",
    "    \"\"\" Function to manage a pool of HTTP workers \"\"\"\n",
    "    global list_of_all_requests, num_responses\n",
    "    with ThreadPoolExecutor(max_workers=pool_size) as executor:\n",
    "        # Chunk the requests into groups of (pool_size * 20)\n",
    "        for _ in range(0, len(list_of_all_requests), pool_size * 20):\n",
    "            futures = [executor.submit(http_worker) for _ in range(pool_size * 20)]\n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                yield future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_segmentation_api = \"https://feta-coriander-ljq4xhb0fbxkxuye.salad.cloud\"\n",
    "\n",
    "def all_requests():\n",
    "    for url in list_all_file_urls(image_bucket, \"coco2017/train2017/\"):\n",
    "        yield {\n",
    "          \"method\": \"GET\",\n",
    "          \"url\": f\"{image_segmentation_api}/segment\",\n",
    "          \"params\": {\"url\": url, \"multimask_output\": False }\n",
    "        }\n",
    "    for url in list_all_file_urls(image_bucket, \"ava/images/\"):\n",
    "        yield {\n",
    "          \"method\": \"GET\",\n",
    "          \"url\": f\"{image_segmentation_api}/segment\",\n",
    "          \"params\": {\"url\": url, \"multimask_output\": False}\n",
    "        }\n",
    "        \n",
    "def get_image_url_from_response(response: requests.Response) -> str:\n",
    "    return parse_qs(urlparse(response.request.url).query)[\"url\"][0]\n",
    "    \n",
    "    \n",
    "def get_rows():\n",
    "    for response in fetch_responses(pool_size=50):\n",
    "        if response is None:\n",
    "            continue\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"Error: {response.status_code} {response.reason}\")\n",
    "        row = {\n",
    "            \"image_url\": get_image_url_from_response(response),\n",
    "        }\n",
    "        salad_headers = [header for header in response.headers if header in [\"x-gpu-name\", \"x-salad-machine-id\", \"x-salad-container-group-id\", \"x-inference-time\", \"x-image-load-time\"]]\n",
    "        for header in salad_headers:\n",
    "            row[header] = response.headers[header]\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of requests: 373795\n"
     ]
    }
   ],
   "source": [
    "request_list_cache_file = \"all_requests.json\"\n",
    "\n",
    "if os.path.exists(request_list_cache_file):\n",
    "    with open(request_list_cache_file) as f:\n",
    "        list_of_all_requests = json.load(f)\n",
    "else:\n",
    "    list_of_all_requests = list(all_requests())\n",
    "    with open(request_list_cache_file, \"w\") as f:\n",
    "        json.dump(list_of_all_requests, f)\n",
    "print(f\"Total number of requests: {len(list_of_all_requests)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming at row #152848\n",
      "Total number of rows: 152848\n"
     ]
    }
   ],
   "source": [
    "row_file = \"all_rows.jsonl\"\n",
    "rows = []\n",
    "if os.path.exists(row_file):\n",
    "    with open(row_file) as f:\n",
    "        rows = [json.loads(line) for line in f]\n",
    "        processed_urls = set([row[\"image_url\"] for row in rows])\n",
    "        print(f\"Resuming at row #{len(rows)}\")\n",
    "        list_of_all_requests = [\n",
    "            request\n",
    "            for request in list_of_all_requests\n",
    "            if request[\"params\"][\"url\"] not in processed_urls\n",
    "        ]\n",
    "# with open(row_file, \"a\") as f:\n",
    "#     for row in get_rows():\n",
    "#         f.write(json.dumps(row) + \"\\n\")\n",
    "#         rows.append(row)\n",
    "\n",
    "print(f\"Total number of rows: {len(rows)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
